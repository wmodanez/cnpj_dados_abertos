# ğŸ¯ Melhores PrÃ¡ticas - Sistema de Processadores RF

## ğŸ“‹ VisÃ£o Geral

Este documento estabelece as melhores prÃ¡ticas para desenvolvimento, manutenÃ§Ã£o e operaÃ§Ã£o do sistema refatorado, garantindo qualidade, performance e sustentabilidade a longo prazo.

**Ãreas Cobertas:**
- âœ… **Desenvolvimento**: PadrÃµes de cÃ³digo e arquitetura
- âœ… **Testes**: EstratÃ©gias de validaÃ§Ã£o e qualidade
- âœ… **Performance**: OtimizaÃ§Ã£o e monitoramento
- âœ… **SeguranÃ§a**: ProteÃ§Ã£o e conformidade
- âœ… **OperaÃ§Ã£o**: Deploy, monitoramento e manutenÃ§Ã£o

## ğŸ’» PrÃ¡ticas de Desenvolvimento

### Estrutura de CÃ³digo

#### OrganizaÃ§Ã£o de Arquivos

```python
# âœ… BOM: Estrutura clara e hierÃ¡rquica
src/
â”œâ”€â”€ process/
â”‚   â”œâ”€â”€ base/           # Infraestrutura central
â”‚   â””â”€â”€ processors/     # Processadores especÃ­ficos
â”œâ”€â”€ Entity/             # Sistema de entidades
â””â”€â”€ utils/              # UtilitÃ¡rios compartilhados

# âŒ RUIM: Estrutura plana e confusa
src/
â”œâ”€â”€ all_processors.py
â”œâ”€â”€ utils_and_entities.py
â””â”€â”€ everything_else.py
```

#### Nomenclatura Consistente

```python
# âœ… BOM: Nomes descritivos e consistentes
class SocioProcessor(BaseProcessor):
    def process_single_zip(self, zip_file: str) -> bool:
        return self._process_zip_internal(zip_file)
    
    def _process_zip_internal(self, zip_file: str) -> bool:
        # ImplementaÃ§Ã£o privada
        pass

# âŒ RUIM: Nomes vagos e inconsistentes
class SP(BaseProcessor):
    def proc(self, f):
        return self.do_stuff(f)
```

#### Type Hints ObrigatÃ³rios

```python
# âœ… BOM: Type hints completos
from typing import List, Dict, Optional, Union
import polars as pl

def process_dataframe(
    df: pl.DataFrame, 
    options: Dict[str, Union[str, bool]]
) -> Optional[pl.DataFrame]:
    """Processa DataFrame com opÃ§Ãµes especÃ­ficas."""
    pass

# âŒ RUIM: Sem type hints
def process_dataframe(df, options):
    pass
```

### PadrÃµes de Design

#### Factory Pattern (JÃ¡ Implementado)

```python
# âœ… BOM: Usar ProcessorFactory
from src.process.base.factory import ProcessorFactory

# Registrar processador
ProcessorFactory.register("custom", CustomProcessor)

# Criar processador
processor = ProcessorFactory.create("custom", zip_dir, unzip_dir, parquet_dir)

# âŒ RUIM: InstanciaÃ§Ã£o direta
processor = CustomProcessor(zip_dir, unzip_dir, parquet_dir)
```

#### Strategy Pattern para TransformaÃ§Ãµes

```python
# âœ… BOM: Strategy pattern para diferentes transformaÃ§Ãµes
class TransformationStrategy(ABC):
    @abstractmethod
    def transform(self, df: pl.DataFrame) -> pl.DataFrame:
        pass

class SimplesTransformation(TransformationStrategy):
    def transform(self, df: pl.DataFrame) -> pl.DataFrame:
        # ConversÃµes S/N especÃ­ficas do Simples
        return df.with_columns([
            pl.when(pl.col("opcao_simples") == "S").then(1).otherwise(0)
        ])

# âŒ RUIM: LÃ³gica hardcoded no processador
def transform_data(self, df):
    if self.processor_type == "simples":
        # cÃ³digo duplicado...
    elif self.processor_type == "empresa":
        # mais cÃ³digo duplicado...
```

#### Dependency Injection

```python
# âœ… BOM: InjeÃ§Ã£o de dependÃªncias
class SocioProcessor(BaseProcessor):
    def __init__(
        self, 
        zip_dir: str, 
        unzip_dir: str, 
        parquet_dir: str,
        resource_monitor: Optional[ResourceMonitor] = None,
        queue_manager: Optional[ProcessingQueueManager] = None
    ):
        super().__init__(zip_dir, unzip_dir, parquet_dir)
        self.resource_monitor = resource_monitor or ResourceMonitor()
        self.queue_manager = queue_manager

# âŒ RUIM: DependÃªncias hardcoded
class SocioProcessor(BaseProcessor):
    def __init__(self, zip_dir, unzip_dir, parquet_dir):
        super().__init__(zip_dir, unzip_dir, parquet_dir)
        self.resource_monitor = ResourceMonitor()  # Acoplamento forte
```

## ğŸ§ª PrÃ¡ticas de Testes

### Estrutura de Testes

```python
# tests/
â”œâ”€â”€ unit/                    # Testes unitÃ¡rios
â”‚   â”œâ”€â”€ test_entities/
â”‚   â”œâ”€â”€ test_processors/
â”‚   â””â”€â”€ test_infrastructure/
â”œâ”€â”€ integration/             # Testes de integraÃ§Ã£o
â”‚   â”œâ”€â”€ test_processor_integration/
â”‚   â””â”€â”€ test_end_to_end/
â”œâ”€â”€ performance/             # Testes de performance
â”‚   â””â”€â”€ test_benchmarks.py
â””â”€â”€ fixtures/                # Dados de teste
    â”œâ”€â”€ sample_socio.zip
    â””â”€â”€ sample_empresa.zip
```

### Testes UnitÃ¡rios

```python
# âœ… BOM: Testes focados e isolados
import pytest
from unittest.mock import Mock, patch
from src.process.processors.socio_processor import SocioProcessor

class TestSocioProcessor:
    @pytest.fixture
    def mock_resource_monitor(self):
        monitor = Mock()
        monitor.get_system_resources_dict.return_value = {
            'cpu_count': 4,
            'memory_available': 8.0,
            'cpu_percent': 50.0
        }
        return monitor
    
    def test_process_single_zip_success(self, mock_resource_monitor):
        """Testa processamento bem-sucedido de ZIP"""
        processor = SocioProcessor("/zip", "/unzip", "/parquet")
        processor.resource_monitor = mock_resource_monitor
        
        with patch('polars.read_csv') as mock_read:
            mock_read.return_value = Mock()
            result = processor.process_single_zip("test.zip")
            
        assert result is True
        mock_read.assert_called_once()

# âŒ RUIM: Testes que dependem de arquivos reais
def test_socio_processor():
    processor = SocioProcessor("/real/path", "/real/unzip", "/real/output")
    result = processor.process_single_zip("/real/file.zip")  # DependÃªncia externa
    assert result
```

### Testes de IntegraÃ§Ã£o

```python
# âœ… BOM: Testes de integraÃ§Ã£o controlados
import tempfile
import pytest
from pathlib import Path

class TestProcessorIntegration:
    @pytest.fixture
    def temp_environment(self):
        """Cria ambiente temporÃ¡rio para testes"""
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            zip_dir = temp_path / "zip"
            unzip_dir = temp_path / "unzip"
            parquet_dir = temp_path / "parquet"
            
            zip_dir.mkdir()
            unzip_dir.mkdir()
            parquet_dir.mkdir()
            
            yield {
                'zip_dir': str(zip_dir),
                'unzip_dir': str(unzip_dir),
                'parquet_dir': str(parquet_dir)
            }
    
    def test_full_processing_pipeline(self, temp_environment):
        """Testa pipeline completo de processamento"""
        # Criar dados de teste
        self.create_test_data(temp_environment['zip_dir'])
        
        # Executar processamento
        from src.process.base.factory import ProcessorFactory
        processor = ProcessorFactory.create("socio", **temp_environment)
        
        result = processor.process_single_zip("test_socio.zip")
        
        # Verificar resultados
        assert result is True
        output_files = list(Path(temp_environment['parquet_dir']).glob("*.parquet"))
        assert len(output_files) > 0
```

### Testes de Performance

```python
# âœ… BOM: Benchmarks automatizados
import pytest
import time
from src.process.processors.socio_processor import SocioProcessor

class TestPerformance:
    @pytest.mark.performance
    def test_socio_processor_benchmark(self, benchmark_data):
        """Benchmark do SocioProcessor"""
        processor = SocioProcessor("/zip", "/unzip", "/parquet")
        
        start_time = time.time()
        result = processor.process_single_zip("benchmark_500_lines.zip")
        end_time = time.time()
        
        processing_time = end_time - start_time
        
        # Assertions de performance
        assert result is True
        assert processing_time < 0.01  # Menos de 10ms
        
        # Log mÃ©tricas
        throughput = 500 / processing_time
        print(f"Throughput: {throughput:.1f} linhas/segundo")

# âŒ RUIM: Testes de performance nÃ£o determinÃ­sticos
def test_performance():
    # Sem controle de ambiente
    start = time.time()
    some_function()
    end = time.time()
    assert end - start < 1  # Pode falhar em mÃ¡quinas lentas
```

## ğŸš€ PrÃ¡ticas de Performance

### Uso Eficiente de Recursos

```python
# âœ… BOM: Monitoramento e controle de recursos
from src.process.base.resource_monitor import ResourceMonitor

class OptimizedProcessor:
    def __init__(self):
        self.monitor = ResourceMonitor()
        
    def process_large_file(self, file_path: str):
        # Verificar recursos antes de processar
        if not self.monitor.can_start_processing(0, 1):
            self.wait_for_resources()
        
        # Processar em chunks baseado na memÃ³ria disponÃ­vel
        chunk_size = self.calculate_optimal_chunk_size()
        
        for chunk in self.read_file_chunks(file_path, chunk_size):
            self.process_chunk(chunk)

# âŒ RUIM: Uso descontrolado de recursos
def process_large_file(file_path):
    # Carregar arquivo inteiro na memÃ³ria
    df = pl.read_csv(file_path)  # Pode causar OOM
    return process_dataframe(df)
```

### Lazy Loading e Streaming

```python
# âœ… BOM: Lazy loading com Polars
def process_large_dataset(file_path: str) -> pl.DataFrame:
    """Processa dataset grande usando lazy evaluation"""
    return (
        pl.scan_csv(file_path)
        .filter(pl.col("cnpj_basico").is_not_null())
        .with_columns([
            pl.col("razao_social").str.upper().alias("razao_social_clean")
        ])
        .select([
            "cnpj_basico",
            "razao_social_clean",
            "situacao_cadastral"
        ])
        .collect()  # Executar apenas quando necessÃ¡rio
    )

# âŒ RUIM: Carregamento eagerly
def process_large_dataset(file_path: str) -> pl.DataFrame:
    df = pl.read_csv(file_path)  # Carrega tudo na memÃ³ria
    df = df.filter(pl.col("cnpj_basico").is_not_null())
    df = df.with_columns([pl.col("razao_social").str.upper()])
    return df.select(["cnpj_basico", "razao_social", "situacao_cadastral"])
```

### Cache Inteligente

```python
# âœ… BOM: Cache com TTL e invalidaÃ§Ã£o
from functools import lru_cache
from datetime import datetime, timedelta
import hashlib

class SmartCache:
    def __init__(self, ttl_minutes: int = 60):
        self.ttl = timedelta(minutes=ttl_minutes)
        self.cache = {}
        self.timestamps = {}
    
    def get_cache_key(self, file_path: str, processor_type: str) -> str:
        """Gera chave de cache baseada no arquivo e processador"""
        file_stat = os.stat(file_path)
        content = f"{file_path}_{processor_type}_{file_stat.st_mtime}_{file_stat.st_size}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, key: str):
        """ObtÃ©m item do cache se ainda vÃ¡lido"""
        if key not in self.cache:
            return None
            
        if datetime.now() - self.timestamps[key] > self.ttl:
            del self.cache[key]
            del self.timestamps[key]
            return None
            
        return self.cache[key]
    
    def set(self, key: str, value):
        """Armazena item no cache"""
        self.cache[key] = value
        self.timestamps[key] = datetime.now()

# âŒ RUIM: Cache sem controle
cache = {}  # Cache global sem invalidaÃ§Ã£o

def process_file(file_path):
    if file_path in cache:
        return cache[file_path]  # Pode retornar dados obsoletos
    
    result = expensive_processing(file_path)
    cache[file_path] = result  # Cache cresce indefinidamente
    return result
```

## ğŸ”’ PrÃ¡ticas de SeguranÃ§a

### ValidaÃ§Ã£o de Entrada

```python
# âœ… BOM: ValidaÃ§Ã£o rigorosa de entrada
from pathlib import Path
from typing import List

class SecureFileProcessor:
    ALLOWED_EXTENSIONS = {'.zip', '.csv'}
    MAX_FILE_SIZE_MB = 2048
    ALLOWED_BASE_PATHS = [
        Path("/secure/data/input"),
        Path("/opt/rf_data/input")
    ]
    
    def validate_file_path(self, file_path: str) -> bool:
        """Valida se arquivo Ã© seguro para processamento"""
        path = Path(file_path).resolve()
        
        # Verificar se estÃ¡ em diretÃ³rio permitido
        is_in_allowed_path = any(
            self._is_subpath(path, allowed_path) 
            for allowed_path in self.ALLOWED_BASE_PATHS
        )
        
        if not is_in_allowed_path:
            raise SecurityError(f"Arquivo fora de diretÃ³rio permitido: {path}")
        
        # Verificar extensÃ£o
        if path.suffix.lower() not in self.ALLOWED_EXTENSIONS:
            raise SecurityError(f"ExtensÃ£o nÃ£o permitida: {path.suffix}")
        
        # Verificar tamanho
        if path.exists():
            size_mb = path.stat().st_size / (1024 * 1024)
            if size_mb > self.MAX_FILE_SIZE_MB:
                raise SecurityError(f"Arquivo muito grande: {size_mb:.1f}MB")
        
        return True
    
    @staticmethod
    def _is_subpath(path: Path, parent: Path) -> bool:
        """Verifica se path Ã© subdiretÃ³rio de parent"""
        try:
            path.relative_to(parent)
            return True
        except ValueError:
            return False

# âŒ RUIM: Sem validaÃ§Ã£o de seguranÃ§a
def process_file(file_path):
    # Aceita qualquer arquivo de qualquer lugar
    return expensive_processing(file_path)
```

### Logs Seguros

```python
# âœ… BOM: Logs sem informaÃ§Ãµes sensÃ­veis
import logging
import re

class SecureLogger:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
    def log_processing_start(self, file_path: str, user_id: str):
        """Log de inÃ­cio de processamento (sanitizado)"""
        safe_file_path = self._sanitize_path(file_path)
        safe_user_id = self._sanitize_user_id(user_id)
        
        self.logger.info(
            f"Iniciando processamento - File: {safe_file_path}, User: {safe_user_id}"
        )
    
    def _sanitize_path(self, path: str) -> str:
        """Remove informaÃ§Ãµes sensÃ­veis do path"""
        return Path(path).name  # Apenas nome do arquivo
    
    def _sanitize_user_id(self, user_id: str) -> str:
        """Ofusca ID do usuÃ¡rio"""
        if len(user_id) > 4:
            return user_id[:2] + "***" + user_id[-2:]
        return "***"

# âŒ RUIM: Logs com informaÃ§Ãµes sensÃ­veis
def process_file(file_path, user_credentials):
    logging.info(f"Processing {file_path} for user {user_credentials}")  # ExpÃµe credenciais
```

### SanitizaÃ§Ã£o de Dados

```python
# âœ… BOM: SanitizaÃ§Ã£o de dados de entrada
import re
from typing import Union

class DataSanitizer:
    # Patterns para limpeza
    CPF_PATTERN = re.compile(r'[^\d]')
    CNPJ_PATTERN = re.compile(r'[^\d]')
    SQL_INJECTION_PATTERN = re.compile(r"[';\"\\]")
    
    @staticmethod
    def sanitize_cpf(cpf: str) -> str:
        """Sanitiza CPF removendo caracteres nÃ£o numÃ©ricos"""
        if not cpf:
            return ""
        
        clean_cpf = DataSanitizer.CPF_PATTERN.sub('', str(cpf))
        
        # Validar comprimento
        if len(clean_cpf) != 11:
            return ""
        
        return clean_cpf
    
    @staticmethod
    def sanitize_text_field(text: str, max_length: int = 255) -> str:
        """Sanitiza campo de texto removendo caracteres perigosos"""
        if not text:
            return ""
        
        # Remover caracteres de SQL injection
        clean_text = DataSanitizer.SQL_INJECTION_PATTERN.sub('', str(text))
        
        # Limitar comprimento
        clean_text = clean_text[:max_length]
        
        # Remover espaÃ§os extras
        clean_text = ' '.join(clean_text.split())
        
        return clean_text.upper()

# âŒ RUIM: Dados nÃ£o sanitizados
def process_company_data(data):
    company_name = data['name']  # Pode conter SQL injection
    cpf = data['cpf']  # Pode ter formataÃ§Ã£o inconsistente
    return save_to_database(company_name, cpf)
```

## ğŸ“Š PrÃ¡ticas de Monitoramento

### MÃ©tricas Estruturadas

```python
# âœ… BOM: MÃ©tricas estruturadas
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, Any

@dataclass
class ProcessingMetrics:
    processor_type: str
    file_name: str
    start_time: datetime
    end_time: datetime
    lines_processed: int
    success: bool
    error_message: str = ""
    
    @property
    def processing_time_seconds(self) -> float:
        return (self.end_time - self.start_time).total_seconds()
    
    @property
    def throughput_lines_per_second(self) -> float:
        if self.processing_time_seconds > 0:
            return self.lines_processed / self.processing_time_seconds
        return 0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'processor_type': self.processor_type,
            'file_name': self.file_name,
            'processing_time_seconds': self.processing_time_seconds,
            'lines_processed': self.lines_processed,
            'throughput_lps': self.throughput_lines_per_second,
            'success': self.success,
            'timestamp': self.start_time.isoformat()
        }

class MetricsCollector:
    def __init__(self):
        self.metrics = []
    
    def record_processing(self, metrics: ProcessingMetrics):
        """Registra mÃ©tricas de processamento"""
        self.metrics.append(metrics)
        
        # Enviar para sistema de monitoramento
        self._send_to_monitoring_system(metrics.to_dict())

# âŒ RUIM: Logs nÃ£o estruturados
def process_file(file_path):
    start = time.time()
    # ... processamento
    end = time.time()
    
    print(f"Processed {file_path} in {end-start}s")  # Log nÃ£o estruturado
```

### Health Checks Detalhados

```python
# âœ… BOM: Health checks abrangentes
from enum import Enum
from dataclasses import dataclass

class HealthStatus(Enum):
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"

@dataclass
class HealthCheck:
    name: str
    status: HealthStatus
    message: str
    details: Dict[str, Any]

class SystemHealthMonitor:
    def __init__(self):
        self.resource_monitor = ResourceMonitor()
        self.factory = ProcessorFactory()
    
    def check_overall_health(self) -> List[HealthCheck]:
        """Executa todos os health checks"""
        checks = [
            self.check_system_resources(),
            self.check_processors(),
            self.check_disk_space(),
            self.check_memory_usage(),
            self.check_dependencies()
        ]
        return checks
    
    def check_system_resources(self) -> HealthCheck:
        """Verifica recursos do sistema"""
        try:
            resources = self.resource_monitor.get_system_resources_dict()
            
            if resources['cpu_percent'] > 90:
                return HealthCheck(
                    "system_resources",
                    HealthStatus.DEGRADED,
                    f"Alto uso de CPU: {resources['cpu_percent']:.1f}%",
                    resources
                )
            
            return HealthCheck(
                "system_resources",
                HealthStatus.HEALTHY,
                "Recursos dentro do normal",
                resources
            )
            
        except Exception as e:
            return HealthCheck(
                "system_resources",
                HealthStatus.UNHEALTHY,
                f"Erro ao verificar recursos: {e}",
                {}
            )

# âŒ RUIM: Health check simplificado demais
def health_check():
    return {"status": "ok"}  # Muito bÃ¡sico
```

## ğŸ”„ PrÃ¡ticas de ManutenÃ§Ã£o

### Versionamento e Changelog

```yaml
# âœ… BOM: Versionamento semÃ¢ntico
# CHANGELOG.md
## [2.1.0] - 2025-06-02

### Added
- Novo EstabelecimentoProcessor com validaÃ§Ã£o de CNPJ completo
- Cache inteligente com TTL configurÃ¡vel
- Health checks detalhados

### Changed
- ResourceMonitor agora suporta thresholds customizados
- Performance melhorada em 300% para arquivos grandes

### Fixed
- CorreÃ§Ã£o de memory leak no processamento em lote
- ValidaÃ§Ã£o de CPF agora funciona com todos os formatos

### Deprecated
- MÃ©todo process_data_file serÃ¡ removido na versÃ£o 3.0

### Security
- ValidaÃ§Ã£o de paths para prevenir directory traversal
- SanitizaÃ§Ã£o de logs para remover dados sensÃ­veis
```

### MigraÃ§Ã£o de Dados

```python
# âœ… BOM: Sistema de migraÃ§Ã£o estruturado
from abc import ABC, abstractmethod

class DataMigration(ABC):
    version_from: str
    version_to: str
    
    @abstractmethod
    def migrate(self, data_path: str) -> bool:
        """Executa migraÃ§Ã£o dos dados"""
        pass
    
    @abstractmethod
    def rollback(self, data_path: str) -> bool:
        """Desfaz migraÃ§Ã£o se necessÃ¡rio"""
        pass

class Migration_v1_to_v2(DataMigration):
    version_from = "1.0"
    version_to = "2.0"
    
    def migrate(self, data_path: str) -> bool:
        """Migra formato de dados da v1 para v2"""
        try:
            # Backup dos dados originais
            self._create_backup(data_path)
            
            # Aplicar transformaÃ§Ãµes
            self._convert_file_format(data_path)
            self._update_schema(data_path)
            
            # Validar resultado
            if self._validate_migration(data_path):
                self._cleanup_backup(data_path)
                return True
            else:
                self.rollback(data_path)
                return False
                
        except Exception as e:
            logging.error(f"Erro na migraÃ§Ã£o: {e}")
            self.rollback(data_path)
            return False

# âŒ RUIM: MigraÃ§Ã£o manual sem estrutura
def migrate_data():
    # Script manual sem rollback ou validaÃ§Ã£o
    for file in files:
        process_file(file)  # Sem backup, sem validaÃ§Ã£o
```

### DocumentaÃ§Ã£o Viva

```python
# âœ… BOM: DocumentaÃ§Ã£o integrada ao cÃ³digo
class EmpresaProcessor(BaseProcessor):
    """
    Processador especializado para dados de empresas.
    
    Funcionalidades:
    - ExtraÃ§Ã£o automÃ¡tica de CPF da razÃ£o social
    - ValidaÃ§Ã£o de CNPJ bÃ¡sico (8 dÃ­gitos)
    - OpÃ§Ã£o create_private para subset de empresas privadas
    - ClassificaÃ§Ã£o automÃ¡tica de porte empresarial
    
    Performance:
    - ~250 linhas/segundo em hardware mÃ©dio
    - Uso de memÃ³ria: ~25MB por 10.000 registros
    - Suporte a arquivos de atÃ© 2GB
    
    Exemplo:
        >>> from src.process.base.factory import ProcessorFactory
        >>> processor = ProcessorFactory.create("empresa", "/zip", "/unzip", "/out")
        >>> success = processor.process_single_zip("empresa.zip", create_private=True)
        >>> print(f"Processamento {'bem-sucedido' if success else 'falhou'}")
        
    Veja tambÃ©m:
        - BaseProcessor: Classe base com funcionalidades comuns
        - Empresa: Entidade para validaÃ§Ã£o e transformaÃ§Ã£o de dados
    """
    
    def process_single_zip(self, zip_file: str, create_private: bool = False) -> bool:
        """
        Processa arquivo ZIP de empresas.
        
        Args:
            zip_file: Caminho para arquivo ZIP contendo dados CSV
            create_private: Se True, cria subset apenas com empresas privadas
            
        Returns:
            True se processamento foi bem-sucedido, False caso contrÃ¡rio
            
        Raises:
            FileNotFoundError: Se arquivo ZIP nÃ£o existir
            SecurityError: Se arquivo nÃ£o passar na validaÃ§Ã£o de seguranÃ§a
            
        Note:
            Arquivos de saÃ­da sÃ£o salvos em formato Parquet no diretÃ³rio configurado.
            Empresas com natureza jurÃ­dica pÃºblica sÃ£o filtradas quando create_private=True.
        """
        # ImplementaÃ§Ã£o...

# âŒ RUIM: CÃ³digo sem documentaÃ§Ã£o
class EmpresaProcessor(BaseProcessor):
    def process_single_zip(self, zip_file, create_private=False):
        # Sem documentaÃ§Ã£o, parÃ¢metros nÃ£o explicados
        pass
```

## ğŸ“‹ Checklist de Qualidade

### Code Review Checklist

- [ ] âœ… **Type hints**: Todos os mÃ©todos tÃªm type hints
- [ ] âœ… **DocumentaÃ§Ã£o**: Docstrings em classes e mÃ©todos pÃºblicos
- [ ] âœ… **Testes**: Cobertura de testes > 90%
- [ ] âœ… **Performance**: Sem operaÃ§Ãµes bloqueantes desnecessÃ¡rias
- [ ] âœ… **SeguranÃ§a**: ValidaÃ§Ã£o de entrada implementada
- [ ] âœ… **Logs**: Logs estruturados sem dados sensÃ­veis
- [ ] âœ… **Recursos**: Uso controlado de memÃ³ria e CPU
- [ ] âœ… **PadrÃµes**: Seguindo padrÃµes de design estabelecidos

### Deploy Checklist

- [ ] âœ… **Testes passando**: 100% dos testes unitÃ¡rios e integraÃ§Ã£o
- [ ] âœ… **Performance**: Benchmarks dentro dos limites esperados
- [ ] âœ… **ConfiguraÃ§Ã£o**: ConfiguraÃ§Ãµes de produÃ§Ã£o validadas
- [ ] âœ… **Monitoramento**: Health checks funcionando
- [ ] âœ… **Backup**: EstratÃ©gia de backup testada
- [ ] âœ… **Rollback**: Plano de rollback documentado e testado
- [ ] âœ… **DocumentaÃ§Ã£o**: Changelog atualizado
- [ ] âœ… **Alertas**: Alertas configurados para mÃ©tricas crÃ­ticas

### ManutenÃ§Ã£o Checklist

- [ ] âœ… **DependÃªncias**: DependÃªncias atualizadas e seguras
- [ ] âœ… **Logs**: Logs rotacionando corretamente
- [ ] âœ… **MÃ©tricas**: MÃ©tricas sendo coletadas
- [ ] âœ… **Performance**: Performance dentro dos SLAs
- [ ] âœ… **Capacidade**: Recursos suficientes para demanda
- [ ] âœ… **Backups**: Backups funcionando e testados
- [ ] âœ… **SeguranÃ§a**: Patches de seguranÃ§a aplicados
- [ ] âœ… **DocumentaÃ§Ã£o**: DocumentaÃ§Ã£o atualizada

---

**ğŸ’¡ Estas melhores prÃ¡ticas garantem que o sistema refatorado mantenha alta qualidade, performance e seguranÃ§a ao longo do tempo, facilitando manutenÃ§Ã£o e evoluÃ§Ã£o futuras.** 