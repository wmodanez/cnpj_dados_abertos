# Processador de Dados CNPJ üè¢

Este projeto automatiza o download, processamento e armazenamento dos dados p√∫blicos de CNPJ disponibilizados pela Receita Federal. Ele foi desenvolvido para ser eficiente, resiliente e f√°cil de usar.

## üìã Fluxo do Processo

O atual pipeline de processamento de dados de CNPJs segue um fluxo estruturado, mas com oportunidades de otimiza√ß√£o:

```mermaid
---
config:
   look: handDrawn
   theme: neutral
   layout: elk
   elk:
      direction: DOWN
      nodeSpacing: 50
      rankSpacing: 70
      mergeEdges: true
      nodePlacementStrategy: SIMPLE
      algorithm: layered
      layered:
        alignedLayout: true

---
flowchart TD
    %% Etapa 1: Coleta de Dados
    A[In√≠cio] --> B[Configura√ß√£o das vari√°veis]
    B --> C[Inicializa√ß√£o do Dask Cluster]
    C --> D[Busca URL mais recente]
    
    %% Verifica√ß√£o inicial de novos dados
    D --> N{Novos dados dispon√≠veis?}
    N -->|N√£o| Q[Fim]
    
    %% Etapa 2: Download e Descompacta√ß√£o
    N -->|Sim| E{Loop por tipos de dados}
    E --> F[Download dos arquivos ZIP com PyCurl]
    F --> G[Descompacta√ß√£o com zipfile]
    
    %% Etapa 3: Processamento com Dask
    G --> H[Leitura dos CSV com Dask DataFrame]
    H --> I[Transforma√ß√£o e limpeza]
    
    %% Etapa 4: Segmenta√ß√£o de dados
    I --> J[Processamento espec√≠fico do tipo de dado]
    
    %% Etapa 5: Armazenamento em Parquet
    J --> K[Convers√£o para Parquet]
    
    %% Pr√≥ximo tipo de dado ou finaliza√ß√£o
    K --> L{Mais tipos?}
    L -->|Sim| E
    
    %% Consolida√ß√£o dos arquivos Parquet em DuckDB
    L -->|N√£o| R[Leitura e verifica√ß√£o dos arquivos Parquet com Python/Pandas]
    R --> S{Existem arquivos Parquet suficientes?}
    S -->|Sim| O[Consolida√ß√£o dos Parquets em um banco DuckDB]
    S -->|N√£o| P
    
    %% Finaliza√ß√£o
    O --> P[Encerramento do cliente Dask]
    P --> Q
    
    %% Estilos
    classDef ferramenta fill:#f9f,stroke:#333,stroke-width:2px
    classDef process fill:#bbf,stroke:#333,stroke-width:1px
    classDef decision fill:#fbb,stroke:#333,stroke-width:1px
    
    class C,F,G,H,I,J,K,O,R ferramenta
    class B,D,P process
    class E,L,N,S decision
```

### Etapas do Fluxo Atual

1. **Configura√ß√£o e Inicializa√ß√£o**
   - Carregamento de vari√°veis de ambiente com `dotenv`
   - Inicializa√ß√£o do cluster `Dask` para processamento distribu√≠do
   - Configura√ß√£o de logging para acompanhamento do processo

2. **Obten√ß√£o e Extra√ß√£o dos Dados**
   - Uso de `requests` e `BeautifulSoup` para identificar URLs mais recentes
   - Download sequencial de arquivos ZIP usando `PyCurl`
   - Extra√ß√£o dos arquivos com o m√≥dulo `zipfile` do Python

3. **Processamento dos Dados**
   - Leitura dos CSVs extra√≠dos utilizando `Dask DataFrame`
   - Processamento separado para cada tipo de dado (Empresas, Estabelecimentos, Simples, S√≥cios)
   - Transforma√ß√µes e limpezas espec√≠ficas para cada conjunto

4. **Armazenamento Intermedi√°rio**
   - Convers√£o para formato `Parquet` usando `Dask.to_parquet()`
   - Organiza√ß√£o em diret√≥rios por m√™s/ano e tipo de dado

5. **Consolida√ß√£o em Banco Anal√≠tico**
   - Verifica√ß√£o dos arquivos Parquet gerados para cada tipo de dado
   - Jun√ß√£o de todos os arquivos Parquet em um √∫nico banco DuckDB
   - Cria√ß√£o de tabelas, views e otimiza√ß√µes para an√°lise

6. **Finaliza√ß√£o**
   - Encerramento do cliente Dask
   - Gera√ß√£o de logs de conclus√£o

### Ferramentas Utilizadas Atualmente

- **Processamento distribu√≠do:** Dask
- **Download:** PyCurl, requests
- **Parsing HTML:** BeautifulSoup
- **Armazenamento:** Parquet (via Dask)
- **Banco de dados anal√≠tico:** DuckDB

## ‚ú® Caracter√≠sticas

- **Download Paralelo**: Baixa m√∫ltiplos arquivos simultaneamente
- **Sistema de Cache**: Evita baixar novamente arquivos recentemente processados
- **Verifica√ß√£o de Espa√ßo em Disco**: Garante espa√ßo suficiente antes do processamento
- **Verifica√ß√£o de Conex√£o**: Verifica conectividade com a internet antes dos downloads
- **Tratamento Espec√≠fico de Exce√ß√µes**: Melhor robustez e recupera√ß√£o de falhas
- **Paraleliza√ß√£o do Processamento**: Processamento eficiente de arquivos CSV usando Dask e ThreadPoolExecutor
- **Resili√™ncia**: Sistema de retry autom√°tico em caso de falhas
- **Processamento Eficiente**: Utiliza Dask para processamento paralelo
- **Armazenamento Otimizado**: Dados em formato Parquet e DuckDB
- **Logging Detalhado**: Rastreamento completo das opera√ß√µes
- **Configur√°vel**: F√°cil adapta√ß√£o √†s necessidades espec√≠ficas

## üöÄ Como Usar

### Pr√©-requisitos

- Python 3.8 ou superior
- Espa√ßo em disco suficiente para os arquivos
- Conex√£o com internet est√°vel

### Instala√ß√£o

1. **Clone o reposit√≥rio**
```bash
git clone https://github.com/seu-usuario/cnpj.git
cd cnpj
```

2. **Crie um ambiente virtual**
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

3. **Instale as depend√™ncias**
```bash
pip install -r requirements.txt
```

4. **Configure o ambiente**
   - Copie o arquivo `.env.local.example` para `.env.local`
   - Ajuste as configura√ß√µes conforme necess√°rio:
```env
# URL base dos dados da Receita Federal
URL_ORIGIN=https://dados.rfb.gov.br/CNPJ/

# Diret√≥rios para download e processamento
PATH_ZIP=./download/      # Arquivos ZIP baixados
PATH_UNZIP=./unzip/      # Arquivos extra√≠dos
PATH_PARQUET=./parquet/  # Arquivos Parquet processados

# Configura√ß√µes do banco de dados
FILE_DB_PARQUET=cnpj.duckdb
PATH_REMOTE_PARQUET=//servidor/compartilhado/
```

### Execu√ß√£o

```bash
python main.py
```

### Gerenciamento de Cache

```bash
# Exibir informa√ß√µes sobre arquivos em cache
python cache_manager.py cache-info

# Limpar o cache de downloads
python cache_manager.py clear-cache
```

## üìä O que o Script Faz

1. **Download dos Dados**
   - Identifica os arquivos mais recentes
   - Baixa em paralelo com retry autom√°tico
   - Verifica integridade dos arquivos
   - Mant√©m cache para evitar downloads desnecess√°rios

2. **Processamento**
   - Verifica espa√ßo em disco e conex√£o com a internet
   - Extrai arquivos ZIP sequencialmente
   - Processa dados CSV em paralelo com Dask
   - Gera arquivos Parquet otimizados

3. **Armazenamento**
   - Cria banco de dados DuckDB
   - Organiza dados em tabelas
   - Copia para local remoto

## üìù Logs e Monitoramento

- Logs s√£o gerados em `logs/cnpj_process_YYYYMMDD_HHMMSS.log`
- Dashboard Dask dispon√≠vel em `http://localhost:8787`
- Progresso de downloads exibido em tempo real
- Logs detalhados de erros com tratamento espec√≠fico por tipo de exce√ß√£o

## ‚öôÔ∏è Configura√ß√µes

O arquivo `config.py` permite ajustar:

- **Processamento**
  - N√∫mero de workers Dask (`config.dask.n_workers`)
  - Threads por worker
  - Limite de mem√≥ria

- **Cache**
  - Habilitar/desabilitar cache (`config.cache.enabled`)
  - Diret√≥rio do cache (`config.cache.cache_dir`)
  - Tempo de expira√ß√£o do cache (`config.cache.max_age_days`)

- **Arquivos**
  - Encoding
  - Separador
  - Tipos de dados

- **Banco de Dados**
  - N√∫mero de threads
  - Configura√ß√µes de compress√£o

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas! Por favor:

1. Fa√ßa um fork do projeto
2. Crie uma branch para sua feature
3. Fa√ßa commit das mudan√ßas
4. Push para a branch
5. Abra um Pull Request

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ‚ö†Ô∏è Notas

- O processamento pode levar algumas horas dependendo do hardware
- Requisitos m√≠nimos de espa√ßo em disco:
  - Empresas: 5GB
  - Estabelecimentos: 8GB
  - Simples Nacional: 3GB
- Em caso de falhas, o sistema tentar√° novamente automaticamente
- Verifica√ß√£o de espa√ßo em disco √© realizada antes da descompacta√ß√£o

## Otimiza√ß√µes de Processamento

Este projeto foi otimizado para lidar com grandes volumes de dados de maneira eficiente. 
As seguintes otimiza√ß√µes foram implementadas:

### Processamento sequencial de arquivos ZIP

Em vez de descompactar todos os arquivos de uma vez (o que poderia consumir muito espa√ßo em disco), 
o processamento agora √© feito sequencialmente:

1. Cada arquivo ZIP √© descompactado individualmente
2. Os arquivos CSV resultantes s√£o processados em paralelo
3. Os arquivos tempor√°rios s√£o exclu√≠dos imediatamente
4. S√≥ ent√£o o pr√≥ximo arquivo ZIP √© processado

Essa abordagem tem as seguintes vantagens:
- Reduz significativamente o uso de espa√ßo em disco
- Previne vazamentos de mem√≥ria durante o processamento
- Mant√©m o diret√≥rio de trabalho limpo
- Permite processamento de conjuntos de dados maiores sem esgotar o armazenamento

### Sistema de Cache para Downloads

- Evita baixar novamente arquivos j√° processados recentemente
- Configur√°vel via par√¢metros de tempo de expira√ß√£o
- Fornece comandos para gerenciar o cache (visualizar informa√ß√µes e limpar)

### Paraleliza√ß√£o do Processamento de CSV

- Os arquivos CSV dentro de cada ZIP s√£o processados em paralelo
- Utiliza ThreadPoolExecutor e Dask para processamento eficiente
- N√∫mero de workers configur√°vel via `config.dask.n_workers`

### Tratamento Espec√≠fico de Exce√ß√µes

- Implementado tratamento espec√≠fico para diferentes tipos de exce√ß√µes
- Mensagens de erro detalhadas para facilitar a depura√ß√£o
- Melhor robustez e recupera√ß√£o de falhas

### Verifica√ß√µes de Seguran√ßa

- Verifica√ß√£o de espa√ßo em disco antes de iniciar o processamento
- Verifica√ß√£o de espa√ßo antes de descompactar cada arquivo ZIP
- Verifica√ß√£o de conex√£o com a internet antes de iniciar downloads
- Estimativa do tamanho de arquivos ap√≥s descompacta√ß√£o

### Limpeza de arquivos tempor√°rios

Todos os arquivos tempor√°rios descompactados s√£o exclu√≠dos ap√≥s o processamento, mesmo em caso de erro,
garantindo que n√£o fiquem arquivos residuais no sistema.
