# Processador de Dados CNPJ üè¢

Este projeto automatiza o download, processamento e armazenamento dos dados p√∫blicos de CNPJ disponibilizados pela Receita Federal. Ele foi desenvolvido para ser eficiente, resiliente e f√°cil de usar.

## üìã Fluxo do Processo

O atual pipeline de processamento de dados de CNPJs segue um fluxo estruturado, mas com oportunidades de otimiza√ß√£o:

```mermaid
---
config:
   look: handDrawn
   theme: neutral
   layout: elk
   elk:
      direction: DOWN
      nodeSpacing: 50
      rankSpacing: 70
      mergeEdges: true
      nodePlacementStrategy: SIMPLE
      algorithm: layered
      layered:
        alignedLayout: true

---
flowchart TD
    %% Etapa 1: Coleta de Dados
    A[In√≠cio] --> B[Configura√ß√£o das vari√°veis]
    B --> C[Inicializa√ß√£o do Dask Cluster]
    C --> D[Busca URL mais recente]
    
    %% Verifica√ß√£o inicial de novos dados
    D --> N{Novos dados dispon√≠veis?}
    N -->|N√£o| Q[Fim]
    
    %% Etapa 2: Download e Descompacta√ß√£o
    N -->|Sim| E{Loop por tipos de dados}
    E --> F[Download dos arquivos ZIP com PyCurl]
    F --> G[Descompacta√ß√£o com zipfile]
    
    %% Etapa 3: Processamento com Dask
    G --> H[Leitura dos CSV com Dask DataFrame]
    H --> I[Transforma√ß√£o e limpeza]
    
    %% Etapa 4: Segmenta√ß√£o de dados
    I --> J[Processamento espec√≠fico do tipo de dado]
    
    %% Etapa 5: Armazenamento em Parquet
    J --> K[Convers√£o para Parquet]
    
    %% Pr√≥ximo tipo de dado ou finaliza√ß√£o
    K --> L{Mais tipos?}
    L -->|Sim| E
    
    %% Consolida√ß√£o dos arquivos Parquet em DuckDB
    L -->|N√£o| R[Leitura e verifica√ß√£o dos arquivos Parquet com Python/Pandas]
    R --> S{Existem arquivos Parquet suficientes?}
    S -->|Sim| O[Consolida√ß√£o dos Parquets em um banco DuckDB]
    S -->|N√£o| P
    
    %% Finaliza√ß√£o
    O --> P[Encerramento do cliente Dask]
    P --> Q
    
    %% Estilos
    classDef ferramenta fill:#f9f,stroke:#333,stroke-width:2px
    classDef process fill:#bbf,stroke:#333,stroke-width:1px
    classDef decision fill:#fbb,stroke:#333,stroke-width:1px
    
    class C,F,G,H,I,J,K,O,R ferramenta
    class B,D,P process
    class E,L,N,S decision
```

### Etapas do Fluxo Atual

1. **Configura√ß√£o e Inicializa√ß√£o**
   - Carregamento de vari√°veis de ambiente com `dotenv`
   - Inicializa√ß√£o do cluster `Dask` para processamento distribu√≠do
   - Configura√ß√£o de logging para acompanhamento do processo

2. **Obten√ß√£o e Extra√ß√£o dos Dados**
   - Uso de `requests` e `BeautifulSoup` para identificar URLs mais recentes
   - Download sequencial de arquivos ZIP usando `PyCurl`
   - Extra√ß√£o dos arquivos com o m√≥dulo `zipfile` do Python

3. **Processamento dos Dados**
   - Leitura dos CSVs extra√≠dos utilizando `Dask DataFrame`
   - Processamento separado para cada tipo de dado (Empresas, Estabelecimentos, Simples, S√≥cios)
   - Transforma√ß√µes e limpezas espec√≠ficas para cada conjunto

4. **Armazenamento Intermedi√°rio**
   - Convers√£o para formato `Parquet` usando `Dask.to_parquet()`
   - Organiza√ß√£o em diret√≥rios por m√™s/ano e tipo de dado

5. **Consolida√ß√£o em Banco Anal√≠tico**
   - Verifica√ß√£o dos arquivos Parquet gerados para cada tipo de dado
   - Jun√ß√£o de todos os arquivos Parquet em um √∫nico banco DuckDB
   - Cria√ß√£o de tabelas, views e otimiza√ß√µes para an√°lise

6. **Finaliza√ß√£o**
   - Encerramento do cliente Dask
   - Gera√ß√£o de logs de conclus√£o

### Ferramentas Utilizadas Atualmente

- **Processamento distribu√≠do:** Dask
- **Download:** PyCurl, requests
- **Parsing HTML:** BeautifulSoup
- **Armazenamento:** Parquet (via Dask)
- **Banco de dados anal√≠tico:** DuckDB

## ‚ú® Caracter√≠sticas

- **Download Paralelo**: Baixa m√∫ltiplos arquivos simultaneamente
- **Sistema de Cache**: Evita baixar novamente arquivos recentemente processados
- **Verifica√ß√£o de Espa√ßo em Disco**: Garante espa√ßo suficiente antes do processamento
- **Verifica√ß√£o de Conex√£o**: Verifica conectividade com a internet antes dos downloads
- **Tratamento Espec√≠fico de Exce√ß√µes**: Melhor robustez e recupera√ß√£o de falhas
- **Paraleliza√ß√£o do Processamento**: Processamento eficiente de arquivos CSV usando Dask e ThreadPoolExecutor
- **Resili√™ncia**: Sistema de retry autom√°tico em caso de falhas
- **Processamento Eficiente**: Utiliza Dask para processamento paralelo
- **Armazenamento Otimizado**: Dados em formato Parquet e DuckDB
- **Logging Detalhado**: Rastreamento completo das opera√ß√µes
- **Configur√°vel**: F√°cil adapta√ß√£o √†s necessidades espec√≠ficas

## üìã Sugest√µes de Otimiza√ß√£o

O fluxo de processamento pode ser aprimorado conforme o diagrama e sugest√µes a seguir:

```mermaid
---
config:
   look: handDrawn
   theme: neutral
   layout: elk
   elk:
      direction: DOWN
      nodeSpacing: 70
      rankSpacing: 100
      mergeEdges: false
      spacing: 70
      layeringStrategy: NETWORK_SIMPLEX
      nodePlacementStrategy: BRANDES_KOEPF
      algorithm: layered
      layered:
        alignedLayout: true
        nodePlacement: SIMPLE

---
flowchart TD
    %% Etapa 1: Inicializa√ß√£o e Verifica√ß√£o
    A[In√≠cio] --> B[Configura√ß√£o do ambiente]
    B --> C[Inicializa√ß√£o do Spark]
    C --> D[Verifica√ß√£o de novas vers√µes de dados CNPJ]
    D --> D1{Novos dados CNPJ dispon√≠veis?}
    D1 -->|N√£o| P1[Encerramento do Spark]
    P1 --> Z[Fim]
    
    %% Etapa 2: Prepara√ß√£o com Cache
    D1 -->|Sim| E[Consulta ao cache de metadados]
    E --> E1{√â necess√°rio download completo?}
    E1 -->|Sim| F1[Download paralelo com asyncio]
    E1 -->|N√£o| F2[Download seletivo de arquivos]
    F1 --> G[Descompacta√ß√£o paralela]
    F2 --> G
    G --> H[Atualiza√ß√£o do cache]
    
    %% Etapa 3: Loop de Processamento com PySpark
    H --> T[Loop por tipos de dados]
    T --> I[Leitura e transforma√ß√£o com PySpark]
    
    %% Conex√£o direta para valida√ß√£o
    I --> J[Valida√ß√£o com ferramentas do Spark]
    J --> J1{Dados OK?}
    J1 -->|N√£o| J2[Corre√ß√£o com transforma√ß√µes Spark]
    J2 --> J
    J1 -->|Sim| K[Armazenamento em Parquet otimizado]
    K --> T1{Mais tipos de dados?}
    T1 -->|Sim| T
    
    %% Etapa 4: Consolida√ß√£o e Finaliza√ß√£o
    T1 -->|N√£o| L[Verifica√ß√£o dos arquivos Parquet com Spark]
    L --> L1{Parquets completos?}
    L1 -->|N√£o| P1
    L1 -->|Sim| M[Cria√ß√£o de views no DuckDB via Spark-DuckDB]
    M --> P1
    
    %% Estilos mais claros
    classDef processo fill:#d1e7dd,stroke:#0d6832,stroke-width:1px
    classDef novo fill:#a3cfbb,stroke:#0d6832,stroke-width:2px
    classDef decisao fill:#fff3cd,stroke:#856404,stroke-width:1px
    classDef inicio fill:#cfe2ff,stroke:#084298,stroke-width:1px
    classDef fim fill:#f8d7da,stroke:#842029,stroke-width:1px
    classDef loop fill:#e0cffc,stroke:#6f42c1,stroke-width:1px
    classDef spark fill:#f9d5e5,stroke:#862e9c,stroke-width:1px
    
    class A,B inicio
    class D1,J1,T1,L1 decisao
    class F1,F2,G,J2,M novo
    class E,H processo
    class C,I,J,K,L,P1 spark
    class T loop
    class Z fim
```

### 1. Paraleliza√ß√£o e Desempenho

#### Downloads Ass√≠ncronos
- Implementar downloads paralelos com `asyncio` e `aiohttp`
- Redu√ß√£o de 60-80% no tempo de download total
- Funciona em conjunto com o cache de metadados

#### Descompacta√ß√£o em Paralelo
- Usar `concurrent.futures` para extrair m√∫ltiplos arquivos simultaneamente
- Redu√ß√£o significativa no tempo de extra√ß√£o

#### Cache de Metadados
- Implementar cache de metadados (SQLite ou arquivo JSON)
- Evitar reprocessamento desnecess√°rio, processando apenas o que mudou

### 2. Moderniza√ß√£o das Ferramentas

#### Migra√ß√£o para PySpark
- Implementar PySpark como ferramenta principal de processamento
- Melhor otimizador de consultas
- Ecossistema mais maduro e ampla comunidade
- Integra√ß√£o nativa com diversas ferramentas de big data

#### Formato de Armazenamento Otimizado
- Parquet otimizado via PySpark com compress√£o e estat√≠sticas avan√ßadas
- Melhor compress√£o dos dados
- Leitura mais r√°pida com estat√≠sticas de coluna

#### Valida√ß√£o de Dados Integrada
- Utilizar as ferramentas nativas do Spark para valida√ß√£o
- Schema enforcement do Spark
- Regras de qualidade via Spark SQL
- Tratamento integrado de dados inv√°lidos

### 3. Resili√™ncia e Monitoramento

#### Checkpoints de Recupera√ß√£o
- Utilizar o sistema de checkpoints nativo do Spark
- Capacidade de retomar de falhas sem reprocessamento completo

#### Sistema de Monitoramento
- Utilizar a interface web do Spark e integr√°-la com ferramentas de observabilidade
- Prometheus/Grafana para visualiza√ß√£o

#### Tratamento Avan√ßado de Erros
- Aproveitar o mecanismo de valida√ß√£o do Spark para identificar e corrigir erros
- Corre√ß√£o iterativa durante o processamento

### 4. Arquitetura Geral

#### Pipeline Modular
- Arquitetura em etapas independentes
- Facilidade de manuten√ß√£o e possibilidade de executar apenas partes espec√≠ficas

#### Integra√ß√£o Direta com DuckDB
- Utilizar conectores entre Spark e DuckDB para cria√ß√£o de views diretamente
- Processo mais direto e eficiente de disponibiliza√ß√£o dos dados para an√°lise

## üìä Compara√ß√£o de Tecnologias

| Aspecto | Atual | Sugest√£o | Benef√≠cio |
|---------|-------|----------|-----------|
| Processamento Distribu√≠do | Dask | PySpark | Melhor otimiza√ß√£o, pipeline integrado |
| Formato de Armazenamento | Parquet via Dask | Parquet otimizado via Spark | Melhor compress√£o e desempenho de leitura |
| Download de Arquivos | PyCurl sequencial | asyncio/aiohttp paralelo | Redu√ß√£o de 60-80% no tempo de download |
| Descompacta√ß√£o | zipfile sequencial | concurrent.futures paralelo | Redu√ß√£o significativa no tempo de extra√ß√£o |
| Valida√ß√£o de Dados | M√≠nima | Ferramentas nativas do Spark | Valida√ß√£o integrada ao processamento |
| Recupera√ß√£o de Falhas | Inexistente | Sistema de checkpoints do Spark | Continuidade em caso de interrup√ß√µes |
| Monitoramento | Logs b√°sicos | Interface web do Spark + m√©tricas | Melhor observabilidade |

## üìÖ Plano de Implementa√ß√£o Progressiva

Para implementar estas melhorias de forma gradual e segura:

### Fase 1: Otimiza√ß√µes Imediatas (1-2 semanas)
- Implementar downloads paralelos com asyncio
- Adicionar descompacta√ß√£o em paralelo
- Implementar cache b√°sico de metadados

### Fase 2: Migra√ß√£o para PySpark (2-3 semanas)
- Configurar ambiente Spark
- Adaptar scripts de processamento para PySpark
- Implementar valida√ß√£o de dados com ferramentas do Spark

### Fase 3: Otimiza√ß√£o de Fluxo (2-3 semanas)
- Implementar o loop de processamento com valida√ß√£o e corre√ß√£o
- Adicionar sistema de checkpoints
- Otimizar armazenamento Parquet

### Fase 4: Refinamentos Finais (1-2 semanas)
- Implementar integra√ß√£o otimizada com DuckDB
- Configurar monitoramento e m√©tricas
- Testes de desempenho e ajustes finais

## üöÄ Como Usar

### Pr√©-requisitos

- Python 3.8 ou superior
- Espa√ßo em disco suficiente para os arquivos
- Conex√£o com internet est√°vel

### Instala√ß√£o

1. **Clone o reposit√≥rio**
```bash
git clone https://github.com/seu-usuario/cnpj.git
cd cnpj
```

2. **Crie um ambiente virtual**
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

3. **Instale as depend√™ncias**
```bash
pip install -r requirements.txt
```

4. **Configure o ambiente**
   - Copie o arquivo `.env.local.example` para `.env.local`
   - Ajuste as configura√ß√µes conforme necess√°rio:
```env
# URL base dos dados da Receita Federal
URL_ORIGIN=https://dados.rfb.gov.br/CNPJ/

# Diret√≥rios para download e processamento
PATH_ZIP=./download/      # Arquivos ZIP baixados
PATH_UNZIP=./unzip/      # Arquivos extra√≠dos
PATH_PARQUET=./parquet/  # Arquivos Parquet processados

# Configura√ß√µes do banco de dados
FILE_DB_PARQUET=cnpj.duckdb
PATH_REMOTE_PARQUET=//servidor/compartilhado/
```

### Execu√ß√£o

```bash
python main.py
```

### Gerenciamento de Cache

```bash
# Exibir informa√ß√µes sobre arquivos em cache
python cache_manager.py cache-info

# Limpar o cache de downloads
python cache_manager.py clear-cache
```

## üìä O que o Script Faz

1. **Download dos Dados**
   - Identifica os arquivos mais recentes
   - Baixa em paralelo com retry autom√°tico
   - Verifica integridade dos arquivos
   - Mant√©m cache para evitar downloads desnecess√°rios

2. **Processamento**
   - Verifica espa√ßo em disco e conex√£o com a internet
   - Extrai arquivos ZIP sequencialmente
   - Processa dados CSV em paralelo com Dask
   - Gera arquivos Parquet otimizados

3. **Armazenamento**
   - Cria banco de dados DuckDB
   - Organiza dados em tabelas
   - Copia para local remoto

## üìù Logs e Monitoramento

- Logs s√£o gerados em `logs/cnpj_process_YYYYMMDD_HHMMSS.log`
- Dashboard Dask dispon√≠vel em `http://localhost:8787`
- Progresso de downloads exibido em tempo real
- Logs detalhados de erros com tratamento espec√≠fico por tipo de exce√ß√£o

## ‚öôÔ∏è Configura√ß√µes

O arquivo `config.py` permite ajustar:

- **Processamento**
  - N√∫mero de workers Dask (`config.dask.n_workers`)
  - Threads por worker
  - Limite de mem√≥ria

- **Cache**
  - Habilitar/desabilitar cache (`config.cache.enabled`)
  - Diret√≥rio do cache (`config.cache.cache_dir`)
  - Tempo de expira√ß√£o do cache (`config.cache.max_age_days`)

- **Arquivos**
  - Encoding
  - Separador
  - Tipos de dados

- **Banco de Dados**
  - N√∫mero de threads
  - Configura√ß√µes de compress√£o

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas! Por favor:

1. Fa√ßa um fork do projeto
2. Crie uma branch para sua feature
3. Fa√ßa commit das mudan√ßas
4. Push para a branch
5. Abra um Pull Request

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ‚ö†Ô∏è Notas

- O processamento pode levar algumas horas dependendo do hardware
- Requisitos m√≠nimos de espa√ßo em disco:
  - Empresas: 5GB
  - Estabelecimentos: 8GB
  - Simples Nacional: 3GB
- Em caso de falhas, o sistema tentar√° novamente automaticamente
- Verifica√ß√£o de espa√ßo em disco √© realizada antes da descompacta√ß√£o

## Otimiza√ß√µes de Processamento

Este projeto foi otimizado para lidar com grandes volumes de dados de maneira eficiente. 
As seguintes otimiza√ß√µes foram implementadas:

### Processamento sequencial de arquivos ZIP

Em vez de descompactar todos os arquivos de uma vez (o que poderia consumir muito espa√ßo em disco), 
o processamento agora √© feito sequencialmente:

1. Cada arquivo ZIP √© descompactado individualmente
2. Os arquivos CSV resultantes s√£o processados em paralelo
3. Os arquivos tempor√°rios s√£o exclu√≠dos imediatamente
4. S√≥ ent√£o o pr√≥ximo arquivo ZIP √© processado

Essa abordagem tem as seguintes vantagens:
- Reduz significativamente o uso de espa√ßo em disco
- Previne vazamentos de mem√≥ria durante o processamento
- Mant√©m o diret√≥rio de trabalho limpo
- Permite processamento de conjuntos de dados maiores sem esgotar o armazenamento

### Sistema de Cache para Downloads

- Evita baixar novamente arquivos j√° processados recentemente
- Configur√°vel via par√¢metros de tempo de expira√ß√£o
- Fornece comandos para gerenciar o cache (visualizar informa√ß√µes e limpar)

### Paraleliza√ß√£o do Processamento de CSV

- Os arquivos CSV dentro de cada ZIP s√£o processados em paralelo
- Utiliza ThreadPoolExecutor e Dask para processamento eficiente
- N√∫mero de workers configur√°vel via `config.dask.n_workers`

### Tratamento Espec√≠fico de Exce√ß√µes

- Implementado tratamento espec√≠fico para diferentes tipos de exce√ß√µes
- Mensagens de erro detalhadas para facilitar a depura√ß√£o
- Melhor robustez e recupera√ß√£o de falhas

### Verifica√ß√µes de Seguran√ßa

- Verifica√ß√£o de espa√ßo em disco antes de iniciar o processamento
- Verifica√ß√£o de espa√ßo antes de descompactar cada arquivo ZIP
- Verifica√ß√£o de conex√£o com a internet antes de iniciar downloads
- Estimativa do tamanho de arquivos ap√≥s descompacta√ß√£o

### Limpeza de arquivos tempor√°rios

Todos os arquivos tempor√°rios descompactados s√£o exclu√≠dos ap√≥s o processamento, mesmo em caso de erro,
garantindo que n√£o fiquem arquivos residuais no sistema.
