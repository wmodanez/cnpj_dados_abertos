# Processador de Dados CNPJ üè¢

Este projeto automatiza o download, processamento e armazenamento dos dados p√∫blicos de CNPJ disponibilizados pela Receita Federal. Ele foi desenvolvido para ser eficiente, resiliente e f√°cil de usar.

## Navega√ß√£o

<details>
  <summary>üöÄ Como Usar</summary>
  
  - [Como Usar](#-como-usar)
  - [Pr√©-requisitos](#pr√©-requisitos)
  - [Instala√ß√£o](#instala√ß√£o)
  - [Execu√ß√£o](#execu√ß√£o)
  - [Gerenciamento de Cache](#gerenciamento-de-cache)
  - [O que o Script Faz](#-o-que-o-script-faz)
</details>

<details>
  <summary>üìã Fluxo do Processo</summary>
  
  - [Fluxo do Processo](#-fluxo-do-processo)
  - [Etapas do Fluxo Atual](#etapas-do-fluxo-atual)
  - [Ferramentas Utilizadas Atualmente](#ferramentas-utilizadas-atualmente)
</details>

<details>
  <summary>‚ú® Caracter√≠sticas</summary>
  
  - [Caracter√≠sticas](#-caracter√≠sticas)
</details>

<details>
  <summary>üìã Sugest√µes de Otimiza√ß√£o</summary>
  
  - [Sugest√µes de Otimiza√ß√£o](#-sugest√µes-de-otimiza√ß√£o)
  - [1. Paraleliza√ß√£o e Desempenho](#1-paraleliza√ß√£o-e-desempenho)
  - [2. Moderniza√ß√£o das Ferramentas](#2-moderniza√ß√£o-das-ferramentas)
  - [3. Resili√™ncia e Monitoramento](#3-resili√™ncia-e-monitoramento)
  - [4. Arquitetura Geral](#4-arquitetura-geral)
</details>

<details>
  <summary>üìä Compara√ß√£o e Implementa√ß√£o</summary>
  
  - [Compara√ß√£o de Tecnologias](#-compara√ß√£o-de-tecnologias)
  - [Plano de Implementa√ß√£o Progressiva](#-plano-de-implementa√ß√£o-progressiva)
  - [Diagrama de Gantt do Plano de Implementa√ß√£o](#diagrama-de-gantt-do-plano-de-implementa√ß√£o)
  - [Tabela de Implementa√ß√£o das Branches](#tabela-de-implementa√ß√£o-das-branches)
</details>

<details>
  <summary>üìù Monitoramento e Configura√ß√£o</summary>
  
  - [Logs e Monitoramento](#-logs-e-monitoramento)
  - [Configura√ß√µes](#Ô∏è-configura√ß√µes)
</details>

<details>
  <summary>‚ö° Otimiza√ß√µes de Processamento</summary>
  
  - [Otimiza√ß√µes de Processamento](#otimiza√ß√µes-de-processamento)
  - [Processamento sequencial de arquivos ZIP](#processamento-sequencial-de-arquivos-zip)
  - [Sistema de Cache para Downloads](#sistema-de-cache-para-downloads)
  - [Paraleliza√ß√£o do Processamento de CSV](#paraleliza√ß√£o-do-processamento-de-csv)
  - [Tratamento Espec√≠fico de Exce√ß√µes](#tratamento-espec√≠fico-de-exce√ß√µes)
  - [Verifica√ß√µes de Seguran√ßa](#verifica√ß√µes-de-seguran√ßa)
  - [Limpeza de arquivos tempor√°rios](#limpeza-de-arquivos-tempor√°rios)
</details>

<details>
  <summary>ü§ù Contribui√ß√£o e Licen√ßa</summary>
  
  - [Contribuindo](#-contribuindo)
  - [Licen√ßa](#-licen√ßa)
  - [Notas](#Ô∏è-notas)
</details>

## üöÄ Como Usar

### Pr√©-requisitos

- Python 3.8 ou superior
- Espa√ßo em disco suficiente para os arquivos
- Conex√£o com internet est√°vel

### Instala√ß√£o

1. **Clone o reposit√≥rio**
```bash
git clone https://github.com/seu-usuario/cnpj.git
cd cnpj
```

2. **Crie um ambiente virtual**
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

3. **Instale as depend√™ncias**
```bash
pip install -r requirements.txt
```

4. **Configure o ambiente**
   - Copie o arquivo `.env.local.example` para `.env.local`
   - Ajuste as configura√ß√µes conforme necess√°rio:
```env
# URL base dos dados da Receita Federal
URL_ORIGIN=https://dados.rfb.gov.br/CNPJ/

# Diret√≥rios para download e processamento
PATH_ZIP=./download/      # Arquivos ZIP baixados
PATH_UNZIP=./unzip/      # Arquivos extra√≠dos
PATH_PARQUET=./parquet/  # Arquivos Parquet processados

# Configura√ß√µes do banco de dados
FILE_DB_PARQUET=cnpj.duckdb
PATH_REMOTE_PARQUET=//servidor/compartilhado/
```

### Execu√ß√£o

```bash
python main.py
```

### Gerenciamento de Cache

```bash
# Exibir informa√ß√µes sobre arquivos em cache
python -m src.cache_manager cache-info

# Limpar o cache de downloads
python -m src.cache_manager clear-cache
```

## üìä O que o Script Faz

1. **Download dos Dados**
   - Identifica os arquivos mais recentes
   - Baixa em paralelo com retry autom√°tico
   - Verifica integridade dos arquivos
   - Mant√©m cache para evitar downloads desnecess√°rios

2. **Processamento**
   - Verifica espa√ßo em disco e conex√£o com a internet
   - Extrai arquivos ZIP sequencialmente
   - Processa dados CSV em paralelo com Dask
   - Gera arquivos Parquet otimizados

3. **Armazenamento**
   - Cria banco de dados DuckDB
   - Organiza dados em tabelas
   - Copia para local remoto

## üìã Fluxo do Processo

O atual pipeline de processamento de dados de CNPJs segue um fluxo estruturado, mas com oportunidades de otimiza√ß√£o:

```mermaid
---
config:
   theme: neutral
   layout: elk
   elk:
      direction: DOWN
      nodeSpacing: 50
      rankSpacing: 70
      mergeEdges: true
      nodePlacementStrategy: SIMPLE
      algorithm: layered
      layered:
        alignedLayout: true
   flowchart:
      useMaxWidth: true
      curve: basis
      defaultRenderer: elk
      htmlLabels: true
      animate: true

---
flowchart TD
    %% Etapa 1: Coleta de Dados
    A[In√≠cio] --> B[Configura√ß√£o das vari√°veis]
    B --> C[Inicializa√ß√£o do Dask Cluster]
    C --> D[Busca URL mais recente]
    
    %% Verifica√ß√£o inicial de novos dados
    D --> N{Novos dados dispon√≠veis?}
    N -->|N√£o| Q[Fim]
    
    %% Etapa 2: Download e Descompacta√ß√£o
    N -->|Sim| E{Loop por tipos de dados}
    E --> F[Download dos arquivos ZIP com PyCurl]
    F --> G[Descompacta√ß√£o com zipfile]
    
    %% Etapa 3: Processamento com Dask
    G --> H[Leitura dos CSV com Dask DataFrame]
    H --> I[Transforma√ß√£o e limpeza]
    
    %% Etapa 4: Segmenta√ß√£o de dados
    I --> J[Processamento espec√≠fico do tipo de dado]
    
    %% Etapa 5: Armazenamento em Parquet
    J --> K[Convers√£o para Parquet]
    
    %% Pr√≥ximo tipo de dado ou finaliza√ß√£o
    K --> L{Mais tipos?}
    L -->|Sim| E
    
    %% Consolida√ß√£o dos arquivos Parquet em DuckDB
    L -->|N√£o| R[Leitura e verifica√ß√£o dos arquivos Parquet com Python/Pandas]
    R --> S{Existem arquivos Parquet suficientes?}
    S -->|Sim| O[Consolida√ß√£o dos Parquets em um banco DuckDB]
    S -->|N√£o| P
    
    %% Finaliza√ß√£o
    O --> P[Encerramento do cliente Dask]
    P --> Q
    
    %% Estilos
    classDef ferramenta fill:#f9f,stroke:#333,stroke-width:2px
    classDef process fill:#bbf,stroke:#333,stroke-width:1px
    classDef decision fill:#fbb,stroke:#333,stroke-width:1px
    
    class C,F,G,H,I,J,K,O,R ferramenta
    class B,D,P process
    class E,L,N,S decision
    
    %% Anima√ß√£o das setas
    linkStyle default stroke:#00AA00,stroke-width:2px,color:green,animation:flowing 1.5s linear infinite,stroke-dasharray:5,2,3,2
```

### Etapas do Fluxo Atual

1. **Configura√ß√£o e Inicializa√ß√£o**
   - Carregamento de vari√°veis de ambiente com `dotenv`
   - Inicializa√ß√£o do cluster `Dask` para processamento distribu√≠do
   - Configura√ß√£o de logging para acompanhamento do processo

2. **Obten√ß√£o e Extra√ß√£o dos Dados**
   - Uso de `requests` e `BeautifulSoup` para identificar URLs mais recentes
   - Download sequencial de arquivos ZIP usando `PyCurl`
   - Extra√ß√£o dos arquivos com o m√≥dulo `zipfile` do Python

3. **Processamento dos Dados**
   - Leitura dos CSVs extra√≠dos utilizando `Dask DataFrame`
   - Processamento separado para cada tipo de dado (Empresas, Estabelecimentos, Simples, S√≥cios)
   - Transforma√ß√µes e limpezas espec√≠ficas para cada conjunto

4. **Armazenamento Intermedi√°rio**
   - Convers√£o para formato `Parquet` usando `Dask.to_parquet()`
   - Organiza√ß√£o em diret√≥rios por m√™s/ano e tipo de dado

5. **Consolida√ß√£o em Banco Anal√≠tico**
   - Verifica√ß√£o dos arquivos Parquet gerados para cada tipo de dado
   - Jun√ß√£o de todos os arquivos Parquet em um √∫nico banco DuckDB
   - Cria√ß√£o de tabelas, views e otimiza√ß√µes para an√°lise

6. **Finaliza√ß√£o**
   - Encerramento do cliente Dask
   - Gera√ß√£o de logs de conclus√£o

### Ferramentas Utilizadas Atualmente

- **Processamento distribu√≠do:** Dask
- **Download:** PyCurl, requests
- **Parsing HTML:** BeautifulSoup
- **Armazenamento:** Parquet (via Dask)
- **Banco de dados anal√≠tico:** DuckDB

## ‚ú® Caracter√≠sticas

- **Download Paralelo**: Baixa m√∫ltiplos arquivos simultaneamente
- **Sistema de Cache**: Evita baixar novamente arquivos recentemente processados
- **Verifica√ß√£o de Espa√ßo em Disco**: Garante espa√ßo suficiente antes do processamento
- **Verifica√ß√£o de Conex√£o**: Verifica conectividade com a internet antes dos downloads
- **Tratamento Espec√≠fico de Exce√ß√µes**: Melhor robustez e recupera√ß√£o de falhas
- **Paraleliza√ß√£o do Processamento**: Processamento eficiente de arquivos CSV usando Dask e ThreadPoolExecutor
- **Resili√™ncia**: Sistema de retry autom√°tico em caso de falhas
- **Processamento Eficiente**: Utiliza Dask para processamento paralelo
- **Armazenamento Otimizado**: Dados em formato Parquet e DuckDB
- **Logging Detalhado**: Rastreamento completo das opera√ß√µes
- **Configur√°vel**: F√°cil adapta√ß√£o √†s necessidades espec√≠ficas

## üìù Logs e Monitoramento

- Logs s√£o gerados em `logs/cnpj_process_YYYYMMDD_HHMMSS.log`
- Dashboard Dask dispon√≠vel em `http://localhost:8787`
- Progresso de downloads exibido em tempo real
- Logs detalhados de erros com tratamento espec√≠fico por tipo de exce√ß√£o

## ‚öôÔ∏è Configura√ß√µes

O arquivo `config.py` permite ajustar:

- **Processamento**
  - N√∫mero de workers Dask (`config.dask.n_workers`)
  - Threads por worker
  - Limite de mem√≥ria

- **Cache**
  - Habilitar/desabilitar cache (`config.cache.enabled`)
  - Diret√≥rio do cache (`config.cache.cache_dir`)
  - Tempo de expira√ß√£o do cache (`config.cache.max_age_days`)

- **Arquivos**
  - Encoding
  - Separador
  - Tipos de dados

- **Banco de Dados**
  - N√∫mero de threads
  - Configura√ß√µes de compress√£o

## üìã Sugest√µes de Otimiza√ß√£o

O fluxo de processamento pode ser aprimorado conforme o diagrama e sugest√µes a seguir:

```mermaid
---
config:
   theme: neutral
   layout: elk
   elk:
      direction: DOWN
      nodeSpacing: 70
      rankSpacing: 100
      mergeEdges: false
      spacing: 70
      layeringStrategy: NETWORK_SIMPLEX
      nodePlacementStrategy: BRANDES_KOEPF
      algorithm: layered
      layered:
        alignedLayout: true
        nodePlacement: SIMPLE
   flowchart:
      useMaxWidth: true
      curve: basis
      defaultRenderer: elk
      htmlLabels: true
      animate: true

---
flowchart TD
    %% Etapa 1: Inicializa√ß√£o e Verifica√ß√£o
    A[In√≠cio] --> B[Configura√ß√£o do ambiente]
    B --> C[Inicializa√ß√£o do Spark]
    C --> D[Verifica√ß√£o de novas vers√µes de dados CNPJ]
    D --> D1{Novos dados CNPJ dispon√≠veis?}
    D1 -->|N√£o| P1[Encerramento do Spark]
    P1 --> Z[Fim]
    
    %% Etapa 2: Prepara√ß√£o com Cache
    D1 -->|Sim| E[Consulta ao cache de metadados]
    E --> E1{√â necess√°rio download completo?}
    E1 -->|Sim| F1[Download paralelo com asyncio]
    E1 -->|N√£o| F2[Download seletivo de arquivos]
    F1 --> G[Descompacta√ß√£o paralela]
    F2 --> G
    G --> H[Atualiza√ß√£o do cache]
    
    %% Etapa 3: Loop de Processamento com PySpark
    H --> T[Loop por tipos de dados]
    T --> I[Leitura e transforma√ß√£o com PySpark]
    
    %% Conex√£o direta para valida√ß√£o
    I --> J[Valida√ß√£o com ferramentas do Spark]
    J --> J1{Dados OK?}
    J1 -->|N√£o| J2[Corre√ß√£o com transforma√ß√µes Spark]
    J2 --> J
    J1 -->|Sim| K[Armazenamento em Parquet otimizado]
    K --> T1{Mais tipos de dados?}
    T1 -->|Sim| T
    
    %% Etapa 4: Consolida√ß√£o e Finaliza√ß√£o
    T1 -->|N√£o| L[Verifica√ß√£o dos arquivos Parquet com Spark]
    L --> L1{Parquets completos?}
    L1 -->|N√£o| P1
    L1 -->|Sim| M[Cria√ß√£o de views no DuckDB via Spark-DuckDB]
    M --> P1
    
    %% Estilos mais claros
    classDef processo fill:#d1e7dd,stroke:#0d6832,stroke-width:1px
    classDef novo fill:#a3cfbb,stroke:#0d6832,stroke-width:2px
    classDef decisao fill:#fff3cd,stroke:#856404,stroke-width:1px
    classDef inicio fill:#cfe2ff,stroke:#084298,stroke-width:1px
    classDef fim fill:#f8d7da,stroke:#842029,stroke-width:1px
    classDef loop fill:#e0cffc,stroke:#6f42c1,stroke-width:1px
    classDef spark fill:#f9d5e5,stroke:#862e9c,stroke-width:1px
    
    class A,B inicio
    class D1,J1,T1,L1 decisao
    class F1,F2,G,J2,M novo
    class E,H processo
    class C,I,J,K,L,P1 spark
    class T loop
    class Z fim
    
    %% Anima√ß√£o das setas
    linkStyle default stroke:#00AA00,stroke-width:2px,color:green,animation:flowing 2s ease infinite,stroke-dasharray:5,2,3,2
```

### 1. Paraleliza√ß√£o e Desempenho

#### Downloads Ass√≠ncronos
- Implementar downloads paralelos com `asyncio` e `aiohttp`
- Redu√ß√£o de 60-80% no tempo de download total
- Funciona em conjunto com o cache de metadados

```bash
# Criar branch para implementa√ß√£o de downloads ass√≠ncronos
git checkout -b feature/async-downloads master
```

#### Descompacta√ß√£o em Paralelo
- Usar `concurrent.futures` para extrair m√∫ltiplos arquivos simultaneamente
- Redu√ß√£o significativa no tempo de extra√ß√£o

```bash
# Criar branch para implementa√ß√£o de descompacta√ß√£o paralela
git checkout -b feature/parallel-extraction master
```

#### Cache de Metadados
- Implementar cache de metadados (SQLite ou arquivo JSON)
- Evitar reprocessamento desnecess√°rio, processando apenas o que mudou

```bash
# Criar branch para implementa√ß√£o do cache de metadados
git checkout -b feature/metadata-cache master
```

### 2. Moderniza√ß√£o das Ferramentas

#### Migra√ß√£o para PySpark
- Implementar PySpark como ferramenta principal de processamento
- Melhor otimizador de consultas
- Ecossistema mais maduro e ampla comunidade
- Integra√ß√£o nativa com diversas ferramentas de big data

```bash
# Criar branch para migra√ß√£o para PySpark
git checkout -b feature/pyspark-migration master
```

#### Formato de Armazenamento Otimizado
- Parquet otimizado via PySpark com compress√£o e estat√≠sticas avan√ßadas
- Melhor compress√£o dos dados
- Leitura mais r√°pida com estat√≠sticas de coluna

```bash
# Criar branch para implementa√ß√£o de armazenamento otimizado
git checkout -b feature/optimized-storage master
```

#### Valida√ß√£o de Dados Integrada
- Utilizar as ferramentas nativas do Spark para valida√ß√£o
- Schema enforcement do Spark
- Regras de qualidade via Spark SQL
- Tratamento integrado de dados inv√°lidos

```bash
# Criar branch para implementa√ß√£o de valida√ß√£o de dados integrada
git checkout -b feature/integrated-validation master
```

### 3. Resili√™ncia e Monitoramento

#### Checkpoints de Recupera√ß√£o
- Utilizar o sistema de checkpoints nativo do Spark
- Capacidade de retomar de falhas sem reprocessamento completo

```bash
# Criar branch para implementa√ß√£o de checkpoints de recupera√ß√£o
git checkout -b feature/recovery-checkpoints master
```

#### Sistema de Monitoramento
- Utilizar a interface web do Spark e integr√°-la com ferramentas de observabilidade
- Prometheus/Grafana para visualiza√ß√£o

```bash
# Criar branch para implementa√ß√£o do sistema de monitoramento
git checkout -b feature/monitoring-system master
```

#### Tratamento Avan√ßado de Erros
- Aproveitar o mecanismo de valida√ß√£o do Spark para identificar e corrigir erros
- Corre√ß√£o iterativa durante o processamento

```bash
# Criar branch para implementa√ß√£o de tratamento avan√ßado de erros
git checkout -b feature/advanced-error-handling master
```

### 4. Arquitetura Geral

#### Pipeline Modular
- Arquitetura em etapas independentes
- Facilidade de manuten√ß√£o e possibilidade de executar apenas partes espec√≠ficas

```bash
# Criar branch para implementa√ß√£o de pipeline modular
git checkout -b feature/modular-pipeline master
```

#### Integra√ß√£o Direta com DuckDB
- Utilizar conectores entre Spark e DuckDB para cria√ß√£o de views diretamente
- Processo mais direto e eficiente de disponibiliza√ß√£o dos dados para an√°lise

```bash
# Criar branch para implementa√ß√£o de integra√ß√£o com DuckDB
git checkout -b feature/duckdb-integration master
```

## üìä Compara√ß√£o de Tecnologias

| Aspecto | Atual | Sugest√£o | Benef√≠cio |
|---------|-------|----------|-----------|
| Processamento Distribu√≠do | Dask | PySpark | Melhor otimiza√ß√£o, pipeline integrado |
| Formato de Armazenamento | Parquet via Dask | Parquet otimizado via Spark | Melhor compress√£o e desempenho de leitura |
| Download de Arquivos | PyCurl sequencial | asyncio/aiohttp paralelo | Redu√ß√£o de 60-80% no tempo de download |
| Descompacta√ß√£o | zipfile sequencial | concurrent.futures paralelo | Redu√ß√£o significativa no tempo de extra√ß√£o |
| Valida√ß√£o de Dados | M√≠nima | Ferramentas nativas do Spark | Valida√ß√£o integrada ao processamento |
| Recupera√ß√£o de Falhas | Inexistente | Sistema de checkpoints do Spark | Continuidade em caso de interrup√ß√µes |
| Monitoramento | Logs b√°sicos | Interface web do Spark + m√©tricas | Melhor observabilidade |

## üìÖ Plano de Implementa√ß√£o Progressiva

Para implementar estas melhorias de forma gradual e segura:

### Fase 1: Otimiza√ß√µes Imediatas (1-2 semanas)
- Implementar downloads paralelos com asyncio
- Adicionar descompacta√ß√£o em paralelo
- Implementar cache b√°sico de metadados

### Fase 2: Migra√ß√£o para PySpark (2-3 semanas)
- Configurar ambiente Spark
- Adaptar scripts de processamento para PySpark
- Implementar valida√ß√£o de dados com ferramentas do Spark

### Fase 3: Otimiza√ß√£o de Fluxo (2-3 semanas)
- Implementar o loop de processamento com valida√ß√£o e corre√ß√£o
- Adicionar sistema de checkpoints
- Otimizar armazenamento Parquet

### Fase 4: Refinamentos Finais (1-2 semanas)
- Implementar integra√ß√£o otimizada com DuckDB
- Configurar monitoramento e m√©tricas
- Testes de desempenho e ajustes finais

### Tabela de Implementa√ß√£o das Branches

| Fase | Nome da Branch | Descri√ß√£o | Status | Depend√™ncias |
|------|---------------|-----------|--------|--------------|
| 1 | feature/async-downloads | Implementa√ß√£o de downloads ass√≠ncronos | üöß | - |
| 1 | feature/parallel-extraction | Descompacta√ß√£o em paralelo de arquivos | ‚è≥ | - |
| 1 | feature/metadata-cache | Sistema de cache de metadados | ‚è≥ | - |
| 2 | feature/pyspark-migration | Migra√ß√£o do processamento para PySpark | ‚è≥ | Fase 1 |
| 2 | feature/optimized-storage | Otimiza√ß√£o do formato de armazenamento | ‚è≥ | feature/pyspark-migration |
| 2 | feature/integrated-validation | Valida√ß√£o integrada de dados com Spark | ‚è≥ | feature/pyspark-migration |
| 3 | feature/recovery-checkpoints | Sistema de checkpoints para recupera√ß√£o | ‚è≥ | feature/pyspark-migration |
| 3 | feature/monitoring-system | Implementa√ß√£o de sistema de monitoramento | ‚è≥ | feature/pyspark-migration |
| 3 | feature/advanced-error-handling | Tratamento avan√ßado de erros | ‚è≥ | feature/pyspark-migration |
| 4 | feature/modular-pipeline | Implementa√ß√£o de pipeline modular | ‚è≥ | Fase 3 |
| 4 | feature/duckdb-integration | Integra√ß√£o direta com DuckDB | ‚è≥ | Fase 3 |

### Diagrama de Gantt do Plano de Implementa√ß√£o

O diagrama abaixo ilustra a programa√ß√£o temporal das tarefas, suas interdepend√™ncias e o caminho cr√≠tico do projeto de otimiza√ß√£o:

```mermaid
gantt
    title Cronograma de Implementa√ß√£o da Otimiza√ß√£o do Fluxo CNPJ
    dateFormat  YYYY-MM-DD
    axisFormat %d/%m
    excludes weekends 2025-04-17 2025-04-18 2025-04-21 2025-05-01 2025-05-02 2025-06-19 2025-06-20
    
    %% Feriados brasileiros e pontos facultativos
    section Dias N√£o √öteis
    Ponto Facultativo (antes da Paix√£o)       :crit, active, holiday0a, 2025-04-17, 1d
    Sexta-feira da Paix√£o                    :crit, active, holiday0, 2025-04-18, 1d
    Tiradentes                               :crit, active, holiday1, 2025-04-21, 1d
    Dia do Trabalho                          :crit, active, holiday2, 2025-05-01, 1d
    Ponto Facultativo (ap√≥s Dia do Trabalho) :crit, active, holiday2b, 2025-05-02, 1d
    Corpus Christi                           :crit, active, holiday3, 2025-06-19, 1d
    Ponto Facultativo (ap√≥s Corpus Christi)  :crit, active, holiday3b, 2025-06-20, 1d
    
    section Fase 1: Otimiza√ß√µes Imediatas
    An√°lise inicial e planejamento detalhado    :a1, 2025-04-14, 3d
    Implementar downloads paralelos com asyncio  :a2, after a1, 4d
    Adicionar descompacta√ß√£o em paralelo        :a3, after a1, 4d
    Implementar cache b√°sico de metadados       :a4, after a2 a3, 5d
    Testes de performance da Fase 1             :a5, after a4, 2d
    
    section Fase 2: Migra√ß√£o para PySpark
    Configurar ambiente Spark                   :b1, after a5, 3d
    Preparar infraestrutura                     :b2, after b1, 2d
    Adaptar scripts para PySpark                :b3, after b2, 8d
    Implementar valida√ß√£o de dados com Spark    :b4, after b3, 5d
    Testes integrados dos componentes Spark     :b5, after b4, 3d
    
    section Fase 3: Otimiza√ß√£o de Fluxo
    Implementar loop de processamento           :c1, after b5, 6d
    Adicionar sistema de checkpoints            :c2, after c1, 4d
    Otimizar armazenamento Parquet              :c3, after c1, 5d
    Testes de carga do fluxo completo           :c4, after c2 c3, 3d
    
    section Fase 4: Refinamentos Finais
    Implementar integra√ß√£o com DuckDB           :d1, after c4, 4d
    Configurar monitoramento                    :d2, after c4, 3d
    Configurar m√©tricas de desempenho           :d3, after d2, 2d
    Testes finais de desempenho                 :d4, after d1 d3, 3d
    Documenta√ß√£o e treinamento                  :d5, after d4, 2d
```

O diagrama acima representa:

- **Dura√ß√£o das tarefas**: Cada barra representa uma tarefa com sua dura√ß√£o estimada
- **Depend√™ncias**: As tarefas conectadas mostram quais precisam ser conclu√≠das antes de outras come√ßarem
- **Agrupamento**: As tarefas est√£o organizadas nas quatro fases do plano de implementa√ß√£o
- **Caminho cr√≠tico**: A sequ√™ncia de tarefas que determina a dura√ß√£o total do projeto

Este cronograma prev√™ aproximadamente 8-10 semanas para a implementa√ß√£o completa, considerando as depend√™ncias entre tarefas e tempos realistas para desenvolvimento e testes.

## Otimiza√ß√µes de Processamento

Este projeto foi otimizado para lidar com grandes volumes de dados de maneira eficiente. 
As seguintes otimiza√ß√µes foram implementadas:

### Processamento sequencial de arquivos ZIP

Em vez de descompactar todos os arquivos de uma vez (o que poderia consumir muito espa√ßo em disco), 
o processamento agora √© feito sequencialmente:

1. Cada arquivo ZIP √© descompactado individualmente
2. Os arquivos CSV resultantes s√£o processados em paralelo
3. Os arquivos tempor√°rios s√£o exclu√≠dos imediatamente
4. S√≥ ent√£o o pr√≥ximo arquivo ZIP √© processado

Essa abordagem tem as seguintes vantagens:
- Reduz significativamente o uso de espa√ßo em disco
- Previne vazamentos de mem√≥ria durante o processamento
- Mant√©m o diret√≥rio de trabalho limpo
- Permite processamento de conjuntos de dados maiores sem esgotar o armazenamento

### Sistema de Cache para Downloads

- Evita baixar novamente arquivos j√° processados recentemente
- Configur√°vel via par√¢metros de tempo de expira√ß√£o
- Fornece comandos para gerenciar o cache (visualizar informa√ß√µes e limpar)

### Paraleliza√ß√£o do Processamento de CSV

- Os arquivos CSV dentro de cada ZIP s√£o processados em paralelo
- Utiliza ThreadPoolExecutor e Dask para processamento eficiente
- N√∫mero de workers configur√°vel via `config.dask.n_workers`

### Tratamento Espec√≠fico de Exce√ß√µes

- Implementado tratamento espec√≠fico para diferentes tipos de exce√ß√µes
- Mensagens de erro detalhadas para facilitar a depura√ß√£o
- Melhor robustez e recupera√ß√£o de falhas

### Verifica√ß√µes de Seguran√ßa

- Verifica√ß√£o de espa√ßo em disco antes de iniciar o processamento
- Verifica√ß√£o de espa√ßo antes de descompactar cada arquivo ZIP
- Verifica√ß√£o de conex√£o com a internet antes de iniciar downloads
- Estimativa do tamanho de arquivos ap√≥s descompacta√ß√£o

### Limpeza de arquivos tempor√°rios

Todos os arquivos tempor√°rios descompactados s√£o exclu√≠dos ap√≥s o processamento, mesmo em caso de erro,
garantindo que n√£o fiquem arquivos residuais no sistema.

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas! Por favor:

1. Fa√ßa um fork do projeto
2. Crie uma branch para sua feature
3. Fa√ßa commit das mudan√ßas
4. Push para a branch
5. Abra um Pull Request

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ‚ö†Ô∏è Notas

- O processamento pode levar algumas horas dependendo do hardware
- Requisitos m√≠nimos de espa√ßo em disco:
  - Empresas: 5GB
  - Estabelecimentos: 8GB
  - Simples Nacional: 3GB
- Em caso de falhas, o sistema tentar√° novamente automaticamente
- Verifica√ß√£o de espa√ßo em disco √© realizada antes da descompacta√ß√£o
